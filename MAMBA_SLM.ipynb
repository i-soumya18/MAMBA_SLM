{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#hybrid_mamba_trainning.py\n",
    "\n",
    "\"\"\"\n",
    "Hybrid Mamba-Transformer Small Language Model Training Script\n",
    "Optimized for RTX 4060 8GB VRAM - Local Inference Ready\n",
    "Author: AI Assistant for Soumyaranjan Sahoo\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, TrainingArguments, Trainer\n",
    "import json\n",
    "import numpy as np\n",
    "from typing import Optional, List, Dict, Any\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "class MambaBlock(nn.Module):\n",
    "    \"\"\"Simplified Mamba SSM Block for hybrid architecture\"\"\"\n",
    "    def __init__(self, d_model, d_state=16, expand_factor=2):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_state = d_state\n",
    "        self.expand_factor = expand_factor\n",
    "        self.d_inner = d_model * expand_factor\n",
    "\n",
    "        # Linear projections\n",
    "        self.in_proj = nn.Linear(d_model, self.d_inner * 2, bias=False)\n",
    "        self.conv1d = nn.Conv1d(\n",
    "            in_channels=self.d_inner,\n",
    "            out_channels=self.d_inner,\n",
    "            kernel_size=4, # conv_kernel\n",
    "            bias=True,\n",
    "            padding=2,\n",
    "            groups=self.d_inner,\n",
    "        )\n",
    "        self.x_proj = nn.Linear(self.d_inner, d_state * 2, bias=False)\n",
    "        self.dt_proj = nn.Linear(self.d_inner, d_state, bias=True)\n",
    "        self.out_proj = nn.Linear(self.d_inner, d_model, bias=False)\n",
    "\n",
    "        # State space parameters\n",
    "        self.A_log = nn.Parameter(torch.log(torch.arange(1, d_state + 1, dtype=torch.float32)))\n",
    "        self.D = nn.Parameter(torch.ones(self.d_inner))\n",
    "\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, d_model)\n",
    "        B, L, D = x.shape\n",
    "\n",
    "        residual = x\n",
    "        x = self.norm(x)\n",
    "\n",
    "        # Linear projection\n",
    "        xz = self.in_proj(x)  # (B, L, 2*d_inner)\n",
    "        x, z = xz.chunk(2, dim=-1)  # (B, L, d_inner) each\n",
    "\n",
    "        # Convolution\n",
    "        x = x.transpose(1, 2)  # (B, d_inner, L)\n",
    "        x = self.conv1d(x)[:, :, :L]  # causal conv\n",
    "        x = x.transpose(1, 2)  # (B, L, d_inner)\n",
    "\n",
    "        # Activation\n",
    "        x = nn.functional.silu(x)\n",
    "\n",
    "        # SSM step (simplified)\n",
    "        A = -torch.exp(self.A_log.float())  # (d_state,)\n",
    "\n",
    "        # Selective mechanism\n",
    "        x_dbl = self.x_proj(x)  # (B, L, 2*d_state)\n",
    "        delta, B_proj = x_dbl.chunk(2, dim=-1)  # (B, L, d_state) each\n",
    "        delta = nn.functional.softplus(self.dt_proj(x))  # (B, L, d_state)\n",
    "\n",
    "        # Simplified SSM computation (for efficiency)\n",
    "        y = x * self.D + torch.sum(B_proj * delta, dim=-1, keepdim=True)\n",
    "\n",
    "        # Gate and output\n",
    "        y = y * nn.functional.silu(z)\n",
    "        output = self.out_proj(y)\n",
    "\n",
    "        return output + residual\n",
    "\n",
    "class HybridAttentionBlock(nn.Module):\n",
    "    \"\"\"Efficient Transformer attention block\"\"\"\n",
    "    def __init__(self, d_model, n_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_model // n_heads\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.qkv = nn.Linear(d_model, 3 * d_model, bias=False)\n",
    "        self.o_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * d_model, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        # x: (batch, seq_len, d_model)\n",
    "        B, L, D = x.shape\n",
    "\n",
    "        # Self-attention\n",
    "        residual = x\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        qkv = self.qkv(x).reshape(B, L, 3, self.n_heads, self.head_dim)\n",
    "        q, k, v = qkv.permute(2, 0, 3, 1, 4)  # (B, n_heads, L, head_dim)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            scores = scores.masked_fill(attention_mask == 0, -1e9)\n",
    "\n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        attn_output = torch.matmul(attn_weights, v)\n",
    "        attn_output = attn_output.transpose(1, 2).reshape(B, L, D)\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "\n",
    "        x = residual + attn_output\n",
    "\n",
    "        # MLP\n",
    "        residual = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.mlp(x)\n",
    "        x = residual + x\n",
    "\n",
    "        return x\n",
    "\n",
    "class HybridMambaTransformer(nn.Module):\n",
    "    \"\"\"Hybrid Mamba-Transformer Model optimized for efficiency\"\"\"\n",
    "    def __init__(self,\n",
    "                 vocab_size=32000,\n",
    "                 d_model=768,\n",
    "                 n_layers=12,\n",
    "                 n_heads=12,\n",
    "                 d_state=16,\n",
    "                 expand_factor=2,\n",
    "                 dropout=0.1,\n",
    "                 max_seq_length=2048,\n",
    "                 layer_pattern=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.n_layers = n_layers\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "        # Default layer pattern: Mamba for early layers, Transformer for later layers\n",
    "        if layer_pattern is None:\n",
    "            # 70% Mamba, 30% Transformer - optimal for efficiency\n",
    "            mamba_layers = int(n_layers * 0.7)\n",
    "            self.layer_pattern = ['mamba'] * mamba_layers + ['transformer'] * (n_layers - mamba_layers)\n",
    "        else:\n",
    "            self.layer_pattern = layer_pattern\n",
    "\n",
    "        # Token embeddings\n",
    "        self.embed_tokens = nn.Embedding(vocab_size, d_model)\n",
    "        self.embed_positions = nn.Embedding(max_seq_length, d_model)\n",
    "\n",
    "        # Hybrid layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        for layer_type in self.layer_pattern:\n",
    "            if layer_type == 'mamba':\n",
    "                self.layers.append(MambaBlock(d_model, d_state, expand_factor))\n",
    "            else:  # transformer\n",
    "                self.layers.append(HybridAttentionBlock(d_model, n_heads, dropout))\n",
    "\n",
    "        # Output\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "\n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        B, L = input_ids.shape\n",
    "\n",
    "        # Embeddings\n",
    "        positions = torch.arange(L, device=input_ids.device).unsqueeze(0).expand(B, -1)\n",
    "        x = self.embed_tokens(input_ids) + self.embed_positions(positions)\n",
    "\n",
    "        # Process through hybrid layers\n",
    "        for i, (layer, layer_type) in enumerate(zip(self.layers, self.layer_pattern)):\n",
    "            if layer_type == 'mamba':\n",
    "                x = layer(x)\n",
    "            else:  # transformer\n",
    "                # Create causal mask for transformer layers\n",
    "                if attention_mask is None:\n",
    "                    causal_mask = torch.tril(torch.ones(L, L, device=x.device))\n",
    "                    causal_mask = causal_mask.unsqueeze(0).unsqueeze(0)  # (1, 1, L, L)\n",
    "                else:\n",
    "                    causal_mask = attention_mask\n",
    "                x = layer(x, causal_mask)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Shift labels for causal LM\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(shift_logits.view(-1, self.vocab_size), shift_labels.view(-1))\n",
    "\n",
    "        return {'loss': loss, 'logits': logits}\n",
    "\n",
    "    def generate(self, input_ids, max_length=100, temperature=0.8, top_p=0.9):\n",
    "        \"\"\"Simple generation function\"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_length - input_ids.size(1)):\n",
    "                outputs = self.forward(input_ids)\n",
    "                logits = outputs['logits'][:, -1, :] / temperature\n",
    "\n",
    "                # Top-p sampling\n",
    "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "                cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "                sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "                sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "                indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
    "                logits[indices_to_remove] = -float('inf')\n",
    "\n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, 1)\n",
    "                input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
    "\n",
    "                if next_token.item() == tokenizer.eos_token_id:\n",
    "                    break\n",
    "\n",
    "        return input_ids\n",
    "\n",
    "# Training configuration optimized for RTX 4060 8GB\n",
    "class ModelConfig:\n",
    "    # Model architecture - optimized for 8GB VRAM\n",
    "    vocab_size = 32000\n",
    "    d_model = 512  # Reduced from 768 for memory efficiency\n",
    "    n_layers = 8   # Balanced for performance vs memory\n",
    "    n_heads = 8\n",
    "    d_state = 16\n",
    "    expand_factor = 2\n",
    "    dropout = 0.1\n",
    "    max_seq_length = 1024  # Reduced for memory efficiency\n",
    "\n",
    "    # Training hyperparameters\n",
    "    batch_size = 2          # Small batch for 8GB VRAM\n",
    "    gradient_accumulation_steps = 8  # Effective batch size = 16\n",
    "    learning_rate = 5e-4\n",
    "    weight_decay = 0.01\n",
    "    max_steps = 5000       # Adjust based on dataset size\n",
    "    warmup_steps = 500\n",
    "    save_steps = 500\n",
    "    eval_steps = 500\n",
    "\n",
    "    # Memory optimization\n",
    "    fp16 = True            # Use mixed precision\n",
    "    gradient_checkpointing = True\n",
    "    dataloader_num_workers = 2\n",
    "\n",
    "def create_training_dataset():\n",
    "    \"\"\"Create or load your training dataset\"\"\"\n",
    "    # Replace this with your actual dataset\n",
    "    # For demonstration, creating a simple dataset\n",
    "\n",
    "    sample_texts = [\n",
    "        \"The quick brown fox jumps over the lazy dog.\",\n",
    "        \"Artificial intelligence is transforming the world.\",\n",
    "        \"Machine learning models require large datasets for training.\",\n",
    "        \"Python is a versatile programming language.\",\n",
    "        \"Deep learning networks can solve complex problems.\",\n",
    "        # Add more training texts here\n",
    "    ]\n",
    "\n",
    "    return sample_texts\n",
    "\n",
    "def train_model():\n",
    "    \"\"\"Main training function\"\"\"\n",
    "    config = ModelConfig()\n",
    "\n",
    "    # Initialize tokenizer (using LLaMA tokenizer as base)\n",
    "    print(\"Loading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Create model\n",
    "    print(\"Creating hybrid model...\")\n",
    "    model = HybridMambaTransformer(\n",
    "        vocab_size=config.vocab_size,\n",
    "        d_model=config.d_model,\n",
    "        n_layers=config.n_layers,\n",
    "        n_heads=config.n_heads,\n",
    "        d_state=config.d_state,\n",
    "        expand_factor=config.expand_factor,\n",
    "        dropout=config.dropout,\n",
    "        max_seq_length=config.max_seq_length\n",
    "    )\n",
    "\n",
    "    # Model info\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"Model size: ~{total_params * 4 / 1024**3:.2f} GB (FP32)\")\n",
    "\n",
    "    # Move to device\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Enable memory optimizations\n",
    "    if config.gradient_checkpointing:\n",
    "        model.gradient_checkpointing_enable()\n",
    "\n",
    "    # Create dataset\n",
    "    texts = create_training_dataset()\n",
    "\n",
    "    # Tokenize dataset\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(\n",
    "            examples,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=config.max_seq_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "    # Simple dataset class\n",
    "    class TextDataset(Dataset):\n",
    "        def __init__(self, texts, tokenizer, max_length):\n",
    "            self.texts = texts\n",
    "            self.tokenizer = tokenizer\n",
    "            self.max_length = max_length\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.texts)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            text = self.texts[idx]\n",
    "            encoding = self.tokenizer(\n",
    "                text,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                max_length=self.max_length,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            return {\n",
    "                'input_ids': encoding['input_ids'].flatten(),\n",
    "                'attention_mask': encoding['attention_mask'].flatten(),\n",
    "                'labels': encoding['input_ids'].flatten()\n",
    "            }\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = TextDataset(texts, tokenizer, config.max_seq_length)\n",
    "\n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./hybrid-mamba-model',\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=config.batch_size,\n",
    "        gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "        learning_rate=config.learning_rate,\n",
    "        weight_decay=config.weight_decay,\n",
    "        max_steps=config.max_steps,\n",
    "        warmup_steps=config.warmup_steps,\n",
    "        logging_steps=100,\n",
    "        save_steps=config.save_steps,\n",
    "        eval_steps=config.eval_steps,\n",
    "        save_total_limit=3,\n",
    "        prediction_loss_only=True,\n",
    "        fp16=config.fp16,\n",
    "        gradient_checkpointing=config.gradient_checkpointing,\n",
    "        dataloader_num_workers=config.dataloader_num_workers,\n",
    "        remove_unused_columns=False,\n",
    "        report_to=None,  # Disable wandb/tensorboard\n",
    "    )\n",
    "\n",
    "    # Create trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    # Start training\n",
    "    print(\"Starting training...\")\n",
    "    trainer.train()\n",
    "\n",
    "    # Save final model\n",
    "    print(\"Saving model...\")\n",
    "    trainer.save_model('./hybrid-mamba-final')\n",
    "    tokenizer.save_pretrained('./hybrid-mamba-final')\n",
    "\n",
    "    print(\"Training completed!\")\n",
    "    return model, tokenizer\n",
    "\n",
    "def test_model(model, tokenizer):\n",
    "    \"\"\"Test the trained model\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    test_prompts = [\n",
    "        \"The future of artificial intelligence is\",\n",
    "        \"In machine learning, we often use\",\n",
    "        \"Python programming allows us to\"\n",
    "    ]\n",
    "\n",
    "    print(\"\\nTesting model generation:\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    for prompt in test_prompts:\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "\n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "\n",
    "        # Generate\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                inputs['input_ids'],\n",
    "                max_length=100,\n",
    "                temperature=0.8,\n",
    "                top_p=0.9\n",
    "            )\n",
    "\n",
    "        # Decode\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        print(f\"Generated: {generated_text}\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Hybrid Mamba-Transformer Training Script\")\n",
    "    print(\"Optimized for RTX 4060 8GB VRAM\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Train the model\n",
    "    model, tokenizer = train_model()\n",
    "\n",
    "    # Test the model\n",
    "    test_model(model, tokenizer)\n",
    "\n",
    "    print(\"\\nModel files saved to: ./hybrid-mamba-final/\")\n",
    "    print(\"You can now use this model for local inference!\")\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#inference.py\n",
    "\"\"\"\n",
    "Local Inference Script for Hybrid Mamba-Transformer Model\n",
    "Optimized for RTX 4060 8GB VRAM\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "from transformers import AutoTokenizer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Import model architecture (assuming it's in the same directory)\n",
    "from hybrid_mamba_training import HybridMambaTransformer\n",
    "\n",
    "class HybridModelInference:\n",
    "    def __init__(self, model_path=\"./hybrid-mamba-final\", device=None):\n",
    "        \"\"\"Initialize the model for inference\"\"\"\n",
    "\n",
    "        if device is None:\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        else:\n",
    "            self.device = torch.device(device)\n",
    "\n",
    "        print(f\"Loading model on: {self.device}\")\n",
    "\n",
    "        # Load configuration\n",
    "        with open(f\"{model_path}/config.json\", \"r\") as f:\n",
    "            config = json.load(f)\n",
    "\n",
    "        # Load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        # Initialize model\n",
    "        self.model = HybridMambaTransformer(\n",
    "            vocab_size=config[\"vocab_size\"],\n",
    "            d_model=config[\"d_model\"],\n",
    "            n_layers=config[\"n_layers\"],\n",
    "            n_heads=config[\"n_heads\"],\n",
    "            d_state=config[\"d_state\"],\n",
    "            expand_factor=config[\"expand_factor\"],\n",
    "            dropout=config[\"dropout\"],\n",
    "            max_seq_length=config[\"max_seq_length\"],\n",
    "            layer_pattern=config[\"layer_pattern\"]\n",
    "        )\n",
    "\n",
    "        # Load trained weights\n",
    "        try:\n",
    "            checkpoint = torch.load(f\"{model_path}/pytorch_model.bin\", map_location=self.device)\n",
    "            self.model.load_state_dict(checkpoint)\n",
    "            print(\"Model weights loaded successfully!\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"Warning: No trained weights found. Using randomly initialized model.\")\n",
    "\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "        # Model info\n",
    "        total_params = sum(p.numel() for p in self.model.parameters())\n",
    "        print(f\"Model parameters: {total_params:,}\")\n",
    "        print(f\"Estimated memory usage: ~{total_params * 2 / 1024**3:.2f} GB (FP16)\")\n",
    "\n",
    "    def generate_text(self,\n",
    "                     prompt,\n",
    "                     max_length=200,\n",
    "                     temperature=0.8,\n",
    "                     top_p=0.9,\n",
    "                     repetition_penalty=1.1,\n",
    "                     do_sample=True):\n",
    "        \"\"\"Generate text from a prompt\"\"\"\n",
    "\n",
    "        # Tokenize input\n",
    "        inputs = self.tokenizer(\n",
    "            prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=self.model.max_seq_length - max_length\n",
    "        ).to(self.device)\n",
    "\n",
    "        input_length = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Use the model's generate method\n",
    "            generated_ids = self.model.generate(\n",
    "                inputs[\"input_ids\"],\n",
    "                max_length=input_length + max_length,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p\n",
    "            )\n",
    "\n",
    "        # Decode only the new tokens\n",
    "        new_tokens = generated_ids[0][input_length:]\n",
    "        generated_text = self.tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "\n",
    "        return generated_text\n",
    "\n",
    "    def chat_interface(self):\n",
    "        \"\"\"Interactive chat interface\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"Hybrid Mamba-Transformer Chat Interface\")\n",
    "        print(\"Type 'quit' to exit, 'clear' to reset context\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        conversation_history = \"\"\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                user_input = input(\"\\nYou: \").strip()\n",
    "\n",
    "                if user_input.lower() == 'quit':\n",
    "                    print(\"Goodbye!\")\n",
    "                    break\n",
    "\n",
    "                if user_input.lower() == 'clear':\n",
    "                    conversation_history = \"\"\n",
    "                    print(\"Context cleared!\")\n",
    "                    continue\n",
    "\n",
    "                if not user_input:\n",
    "                    continue\n",
    "\n",
    "                # Build prompt with conversation history\n",
    "                if conversation_history:\n",
    "                    prompt = f\"{conversation_history}\\nHuman: {user_input}\\nAssistant: \"\n",
    "                else:\n",
    "                    prompt = f\"Human: {user_input}\\nAssistant: \"\n",
    "\n",
    "                # Generate response\n",
    "                print(\"Assistant: \", end=\"\", flush=True)\n",
    "                response = self.generate_text(\n",
    "                    prompt,\n",
    "                    max_length=150,\n",
    "                    temperature=0.7,\n",
    "                    top_p=0.9\n",
    "                )\n",
    "\n",
    "                print(response)\n",
    "\n",
    "                # Update conversation history (keep it manageable)\n",
    "                conversation_history = f\"{conversation_history}\\nHuman: {user_input}\\nAssistant: {response}\"\n",
    "\n",
    "                # Truncate history if too long\n",
    "                if len(conversation_history) > 2000:\n",
    "                    lines = conversation_history.split('\\n')\n",
    "                    conversation_history = '\\n'.join(lines[-10:])  # Keep last 10 exchanges\n",
    "\n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\nGoodbye!\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "\n",
    "    def benchmark_performance(self):\n",
    "        \"\"\"Benchmark model performance\"\"\"\n",
    "        print(\"\\nRunning performance benchmark...\")\n",
    "\n",
    "        test_prompts = [\n",
    "            \"The future of artificial intelligence\",\n",
    "            \"In the field of machine learning\",\n",
    "            \"Python is a programming language that\",\n",
    "            \"The benefits of renewable energy include\",\n",
    "            \"Space exploration has led to\"\n",
    "        ]\n",
    "\n",
    "        import time\n",
    "\n",
    "        total_time = 0\n",
    "        total_tokens = 0\n",
    "\n",
    "        for i, prompt in enumerate(test_prompts):\n",
    "            print(f\"\\nTest {i+1}/5: {prompt}...\")\n",
    "\n",
    "            start_time = time.time()\n",
    "            response = self.generate_text(prompt, max_length=100, temperature=0.8)\n",
    "            end_time = time.time()\n",
    "\n",
    "            generation_time = end_time - start_time\n",
    "            tokens_generated = len(self.tokenizer.encode(response))\n",
    "            tokens_per_second = tokens_generated / generation_time\n",
    "\n",
    "            print(f\"Generated: {response[:100]}...\")\n",
    "            print(f\"Time: {generation_time:.2f}s, Tokens: {tokens_generated}, Speed: {tokens_per_second:.1f} tokens/s\")\n",
    "\n",
    "            total_time += generation_time\n",
    "            total_tokens += tokens_generated\n",
    "\n",
    "        avg_speed = total_tokens / total_time\n",
    "        print(f\"\\nAverage performance: {avg_speed:.1f} tokens/second\")\n",
    "        print(f\"Total time: {total_time:.2f}s, Total tokens: {total_tokens}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function for inference\"\"\"\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser(description=\"Hybrid Mamba-Transformer Inference\")\n",
    "    parser.add_argument(\"--model_path\", type=str, default=\"./hybrid-mamba-final\",\n",
    "                       help=\"Path to the trained model\")\n",
    "    parser.add_argument(\"--prompt\", type=str, help=\"Text prompt for generation\")\n",
    "    parser.add_argument(\"--max_length\", type=int, default=200,\n",
    "                       help=\"Maximum generation length\")\n",
    "    parser.add_argument(\"--temperature\", type=float, default=0.8,\n",
    "                       help=\"Sampling temperature\")\n",
    "    parser.add_argument(\"--top_p\", type=float, default=0.9,\n",
    "                       help=\"Top-p sampling threshold\")\n",
    "    parser.add_argument(\"--chat\", action=\"store_true\",\n",
    "                       help=\"Start interactive chat interface\")\n",
    "    parser.add_argument(\"--benchmark\", action=\"store_true\",\n",
    "                       help=\"Run performance benchmark\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Initialize model\n",
    "    model = HybridModelInference(args.model_path)\n",
    "\n",
    "    if args.benchmark:\n",
    "        model.benchmark_performance()\n",
    "    elif args.chat:\n",
    "        model.chat_interface()\n",
    "    elif args.prompt:\n",
    "        # Single generation\n",
    "        response = model.generate_text(\n",
    "            args.prompt,\n",
    "            max_length=args.max_length,\n",
    "            temperature=args.temperature,\n",
    "            top_p=args.top_p\n",
    "        )\n",
    "        print(f\"Prompt: {args.prompt}\")\n",
    "        print(f\"Response: {response}\")\n",
    "    else:\n",
    "        print(\"Use --prompt for single generation, --chat for interactive mode, or --benchmark for testing\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "51e86221d0a58d8f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "20b802243271fcf9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
