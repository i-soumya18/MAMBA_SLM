# ğŸ›ï¸ MAMBA_SLM Unified Dashboard

A comprehensive PyQt6-based graphical interface for managing all aspects of the Hybrid Mamba-Transformer project.

![Dashboard](https://img.shields.io/badge/GUI-PyQt6-green)
![Python](https://img.shields.io/badge/Python-3.8%2B-blue)
![License](https://img.shields.io/badge/License-MIT-yellow)

## ğŸŒŸ Features

The unified dashboard provides an intuitive interface for:

### ğŸ“š Dataset Management
- **Multiple Sources**: Load from HuggingFace Hub, local files, or directories
- **Real-time Preview**: View dataset samples before training
- **Configuration**: Set max sequence length, batch size, caching options
- **Supported Formats**: TXT, JSON, JSONL
- **Popular Datasets**: WikiText, C4, OpenWebText, Wikipedia, and more

### ğŸ“ Training Configuration
- **Model Architecture**: Configure hidden size, layers, attention heads
- **Training Parameters**: Epochs, learning rate, warmup steps, gradient accumulation
- **Advanced Optimizations**:
  - âœ… Flash Attention 2 (2-3x speedup)
  - âœ… Mixed Precision (FP16/BF16)
  - âœ… Gradient Checkpointing
  - âœ… Quantization (8-bit/4-bit)
- **Real-time Monitoring**: Live training metrics, loss curves, progress tracking
- **Control Panel**: Start, stop, resume training with one click

### ğŸ¯ LoRA/QLoRA Fine-tuning
- **LoRA Configuration**: Set rank, alpha, dropout, target modules
- **QLoRA Support**: 4-bit quantization + LoRA for efficient fine-tuning
- **Model Selection**: Load pretrained checkpoints easily
- **Weight Management**: Apply, merge, and save LoRA weights
- **Fine-tuning Settings**: Custom epochs, learning rates for adaptation

### ğŸ’¬ Interactive Inference
- **Chat Interface**: Real-time conversation with your model
- **Advanced Sampling**:
  - Nucleus Sampling (top-p, top-k)
  - Beam Search
  - Contrastive Search
  - Greedy Decoding
- **Generation Controls**:
  - Temperature adjustment (slider)
  - Top-p/top-k filtering
  - Repetition penalty
  - Max token length
- **Streaming Output**: Token-by-token generation display
- **Model Loading**: Quick model selection and loading

### ğŸ“¦ Export & Evaluation
- **ONNX Export**:
  - Convert models to ONNX format
  - Dynamic batch/sequence support
  - Optimization for inference
  - Cross-platform deployment
- **Benchmarking**:
  - Measure latency and throughput
  - Memory usage tracking
  - Configurable test parameters
  - Detailed performance metrics

### ğŸ—‚ï¸ Model Management
- **Checkpoint Browser**: View all saved checkpoints
- **Model Information**: Detailed stats and configuration
- **Quick Actions**: Load, delete, manage checkpoints
- **Auto-discovery**: Finds checkpoints in common directories

---

## ğŸš€ Installation

### Prerequisites
- Python 3.8 or higher
- pip package manager

### Step 1: Install Dependencies

```bash
# Install all requirements including PyQt6
pip install -r requirements.txt
```

### Step 2: Install Optional Components

```bash
# For Flash Attention (recommended for training)
pip install flash-attn --no-build-isolation

# For GPU ONNX acceleration
pip install onnxruntime-gpu
```

---

## ğŸ® Usage

### Quick Start

**Option 1: Using Launcher Script**
```bash
python launch_dashboard.py
```

**Option 2: Direct Launch**
```bash
python mamba_dashboard.py
```

**Option 3: Windows Double-Click**
```powershell
# Create a shortcut to launch_dashboard.py
# Double-click to launch
```

### First-Time Setup

1. **Launch Dashboard**: Run `python launch_dashboard.py`
2. **Load Dataset**: Go to "ğŸ“š Dataset" tab
   - Select source (HuggingFace or local files)
   - Configure settings
   - Click "Load Dataset"
3. **Configure Training**: Switch to "ğŸ“ Training" tab
   - Set model architecture
   - Choose optimizations
   - Click "Start Training"

---

## ğŸ“– Tab Guide

### 1. ğŸ“š Dataset Tab

**Purpose**: Load and preview training data

**Workflow**:
```
1. Select Source Type
   â”œâ”€ HuggingFace Dataset â†’ Enter dataset name (e.g., "wikitext")
   â”œâ”€ Local File(s) â†’ Browse to .txt, .json, .jsonl
   â”œâ”€ Local Directory â†’ Select folder with text files
   â””â”€ Custom Mix â†’ Combine multiple sources

2. Configure Settings
   â”œâ”€ Max Sequence Length: 128-4096 tokens
   â”œâ”€ Batch Size: 1-128 samples
   â””â”€ Enable Caching: Speed up repeated loads

3. Load Dataset â†’ View preview and stats
```

**Tips**:
- Start with smaller datasets for testing
- Enable caching for faster iterations
- Preview samples to verify formatting

---

### 2. ğŸ“ Training Tab

**Purpose**: Configure and monitor model training

**Workflow**:
```
1. Model Configuration
   â”œâ”€ Hidden Size: 128-2048 (default: 512)
   â”œâ”€ Layers: 2-32 (default: 8)
   â””â”€ Attention Heads: 2-32 (default: 8)

2. Training Settings
   â”œâ”€ Epochs: 1-1000
   â”œâ”€ Learning Rate: 0.00001-0.1
   â”œâ”€ Warmup Steps: 0-10000
   â””â”€ Gradient Accumulation: 1-64

3. Enable Optimizations
   â”œâ”€ âœ… Flash Attention 2 (2-3x speedup)
   â”œâ”€ âœ… Mixed Precision (FP16/BF16)
   â”œâ”€ âœ… Gradient Checkpointing (save memory)
   â””â”€ âœ… Quantization (8-bit/4-bit)

4. Start Training â†’ Monitor progress
```

**Real-time Monitoring**:
- Progress bar showing epoch completion
- Live loss metrics
- Training speed (tokens/sec)
- Estimated time remaining

**Tips**:
- Use Flash Attention for 2-3x speedup
- Enable gradient accumulation for larger effective batch sizes
- Start with smaller models for testing

---

### 3. ğŸ¯ Fine-tuning Tab

**Purpose**: Efficiently fine-tune with LoRA/QLoRA

**Workflow**:
```
1. Load Pretrained Model
   â””â”€ Browse to checkpoint directory

2. Configure LoRA
   â”œâ”€ Rank (r): 1-256 (default: 8)
   â”œâ”€ Alpha: 1-256 (default: 16)
   â”œâ”€ Dropout: 0.0-0.5 (default: 0.05)
   â””â”€ Target Modules: "q_proj,k_proj,v_proj,o_proj"

3. Choose Method
   â”œâ”€ LoRA: Standard parameter-efficient fine-tuning
   â””â”€ âœ… QLoRA: 4-bit quantization + LoRA (recommended)

4. Fine-tuning Settings
   â”œâ”€ Epochs: 1-100 (default: 3)
   â””â”€ Learning Rate: 0.00001-0.01

5. Apply LoRA â†’ Start Fine-tuning â†’ Merge Weights
```

**Memory Savings**:
- LoRA: Train 0.1-1% of parameters
- QLoRA: 75% less memory vs full fine-tuning
- Can fine-tune 100M model on 2GB VRAM

**Tips**:
- Use QLoRA for maximum memory efficiency
- Lower rank (r) = fewer parameters but less capacity
- Start with rank 8-16 for most tasks

---

### 4. ğŸ’¬ Inference Tab

**Purpose**: Interactive chat and text generation

**Workflow**:
```
1. Load Model
   â””â”€ Click "Load Model" button

2. Configure Generation
   â”œâ”€ Max Tokens: 10-2048
   â”œâ”€ Temperature: 0.1-2.0 (use slider)
   â”œâ”€ Top-p: 0.01-1.00
   â”œâ”€ Top-k: 1-200
   â”œâ”€ Repetition Penalty: 1.0-2.0
   â””â”€ Sampling Method: Nucleus/Beam/Contrastive/Greedy

3. Enable Streaming
   â””â”€ âœ… Real-time token-by-token output

4. Enter Prompt â†’ Click Generate (or press Enter)
```

**Sampling Methods**:
- **Nucleus**: Balanced quality and diversity (recommended)
- **Beam Search**: Higher quality, slower
- **Contrastive**: Reduces repetition
- **Greedy**: Fastest, deterministic

**Parameter Guide**:
- **Temperature**: Higher = more creative, Lower = more focused
- **Top-p**: Probability threshold (0.9 = 90% probability mass)
- **Top-k**: Number of top tokens to consider
- **Repetition Penalty**: Penalize repeated tokens (>1.0)

**Tips**:
- Use streaming for real-time feedback
- Lower temperature (0.7-0.8) for factual text
- Higher temperature (0.9-1.2) for creative writing

---

### 5. ğŸ“¦ Export & Eval Tab

**Purpose**: Export models and benchmark performance

**ONNX Export**:
```
1. Select Model
   â””â”€ Browse to checkpoint directory

2. Configure Export
   â”œâ”€ ONNX Opset: 11-17 (default: 14)
   â”œâ”€ âœ… Optimize for Inference
   â””â”€ âœ… Dynamic Batch/Sequence

3. Export to ONNX
   â””â”€ Generates .onnx file for deployment
```

**Benchmarking**:
```
1. Configure Test
   â”œâ”€ Batch Size: 1-32
   â”œâ”€ Sequence Length: 128-4096
   â””â”€ Iterations: 10-1000

2. Run Benchmark
   â””â”€ View latency, throughput, memory usage
```

**Use Cases**:
- **ONNX Export**: Deploy to edge devices, mobile, web
- **Benchmarking**: Compare configurations, optimize settings

---

### 6. ğŸ—‚ï¸ Models Tab

**Purpose**: Manage checkpoints and model files

**Features**:
- **Auto-discovery**: Scans common checkpoint directories
- **Model Info**: View configuration, training stats
- **Quick Actions**: Load, delete, organize checkpoints
- **Checkpoint Browser**: List all saved models

**Workflow**:
```
1. Click "Refresh" to scan for checkpoints
2. Select checkpoint from list
3. View model information
4. Load or delete as needed
```

---

## ğŸ¨ User Interface

### Modern Dark Theme
- Clean, professional dark interface
- Color-coded tabs for easy navigation
- Responsive layout adapts to window size
- High contrast for readability

### Keyboard Shortcuts
- **Enter** in prompt field â†’ Generate
- **Tab** â†’ Navigate between fields
- **Ctrl+C** â†’ Stop generation (inference tab)

### Visual Feedback
- âœ“ Success indicators
- âŒ Error messages with details
- ğŸ”„ Progress bars and spinners
- ğŸ“Š Real-time metric updates

---

## âš™ï¸ Configuration

### Default Directories
The dashboard looks for files in these locations:

```
./checkpoints/      # Saved model checkpoints
./outputs/          # Training outputs
./models/           # Downloaded models
./cache/            # Dataset cache
```

### Customization
Edit `mamba_dashboard.py` to customize:
- Default parameter values
- Theme colors and styling
- Tab order and names
- Checkpoint search paths

---

## ğŸ› Troubleshooting

### Common Issues

**1. "PyQt6 not found"**
```bash
pip install PyQt6
```

**2. "Dataset loader module not available"**
- Ensure `dataset_loader.py` is in the same directory
- Check Python path includes project directory

**3. "Model failed to load"**
- Verify checkpoint path is correct
- Check model files are not corrupted
- Ensure compatible PyTorch version

**4. Training doesn't start**
- Check dataset is loaded first
- Verify CUDA is available (for GPU training)
- Check sufficient disk space

**5. Inference is slow**
- Enable streaming for responsiveness
- Use quantization (4-bit/8-bit)
- Export to ONNX for faster inference

### Getting Help

1. Check error messages in status labels
2. Review console output for detailed errors
3. Verify all dependencies are installed
4. Ensure project modules are available

---

## ğŸ“Š Performance Tips

### Training
- âœ… Enable Flash Attention (2-3x speedup)
- âœ… Use mixed precision (FP16)
- âœ… Increase gradient accumulation for larger effective batches
- âœ… Enable caching for datasets

### Inference
- âœ… Use quantization (4-bit/8-bit)
- âœ… Export to ONNX for production
- âœ… Reduce max tokens for faster responses
- âœ… Lower beam size in beam search

### Memory
- âœ… Use gradient checkpointing
- âœ… Enable 4-bit quantization
- âœ… Use QLoRA instead of full fine-tuning
- âœ… Reduce batch size if OOM

---

## ğŸ”® Future Enhancements

Planned features:
- [ ] Distributed training support
- [ ] Advanced metrics visualization (graphs, charts)
- [ ] Dataset augmentation tools
- [ ] Model comparison dashboard
- [ ] Hyperparameter tuning automation
- [ ] Export to TensorRT, TFLite
- [ ] Custom theme builder
- [ ] Plugin system for extensions

---

## ğŸ¤ Contributing

Contributions welcome! Areas to improve:
- UI/UX enhancements
- Additional sampling methods
- More export formats
- Performance optimizations
- Documentation improvements

---

## ğŸ“ Technical Details

### Architecture
- **Framework**: PyQt6 (Qt 6.6+)
- **Threading**: QThread for background operations
- **Signals**: Qt signals/slots for async communication
- **Styling**: Custom QSS (Qt Style Sheets)

### Integration
The dashboard integrates with:
- `dataset_loader.py` - Dataset management
- `advanced_sampling.py` - Generation strategies
- `lora_finetuning.py` - Parameter-efficient training
- `quantization.py` - Model compression
- `onnx_export.py` - Model conversion
- `train.py` - Training loop
- `evaluate.py` - Evaluation utilities

### Thread Safety
- Training runs in `TrainingThread`
- Inference runs in `InferenceThread`
- UI updates via Qt signals
- Thread-safe model access

---

## ğŸ“„ License

MIT License - see main project README

---

## ğŸ¯ Quick Reference

### Typical Workflow

**1. Pre-training from Scratch**
```
Dataset Tab â†’ Load WikiText
Training Tab â†’ Configure model â†’ Start Training
Models Tab â†’ Monitor checkpoints
```

**2. Fine-tuning Existing Model**
```
Fine-tuning Tab â†’ Load checkpoint â†’ Configure LoRA â†’ Apply LoRA
Training Tab â†’ Start Fine-tuning
Models Tab â†’ Save fine-tuned model
```

**3. Inference and Testing**
```
Inference Tab â†’ Load Model â†’ Configure sampling
Enter prompts â†’ Generate responses
Export & Eval Tab â†’ Run benchmarks
```

**4. Production Deployment**
```
Export & Eval Tab â†’ Select model â†’ Export to ONNX
Benchmark â†’ Optimize settings
Deploy ONNX model to target platform
```

---

## ğŸŒŸ Key Benefits

âœ… **No Command Line Required**: Everything in GUI
âœ… **Real-time Feedback**: See progress instantly
âœ… **Beginner Friendly**: Tooltips and clear labels
âœ… **Advanced Users**: Full control over all parameters
âœ… **Time Saving**: Configure complex setups in seconds
âœ… **Visual**: See model behavior immediately
âœ… **Integrated**: All features in one place

---

**Made with â¤ï¸ for the MAMBA_SLM project**

*Transform your LLM workflow from complex to convenient!* ğŸš€
